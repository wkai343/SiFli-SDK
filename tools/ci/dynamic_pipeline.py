#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import sys
import yaml
import json
import subprocess
import tempfile
from pathlib import Path

class DynamicPipelineGenerator:
    def __init__(self, config_file='build_config_gcc.yaml'):
        self.config_file = config_file
        self.config = self._load_config()
        self.pipeline_config = {
            'stages': ['build', 'collect'],
            'variables': {
                'GIT_SUBMODULE_STRATEGY': 'recursive',
                'COMPILER': 'gcc',
                'PARALLEL_JOBS': '8'
            }
        }
    
    def _load_config(self):
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            sys.exit(1)
    
    def _create_build_job(self, project_path, board=None, is_common=False):
        """åˆ›å»ºå•ä¸ªæ„å»ºJobé…ç½®"""
        # ç”ŸæˆJobåç§°
        job_name = self._generate_job_name(project_path, board)
        
        # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
        log_name = job_name.replace('build_', '')
        
        # æ„å»ºå‚æ•°
        # å¤„ç†boardå‚æ•°ï¼šsconså‘½ä»¤éœ€è¦å»æ‰_hcpuåç¼€
        scons_board = board.replace('_hcpu', '') if board and '_hcpu' in board else board
        build_args = f"--board={scons_board}" if (is_common and scons_board) else ""
        project_type = "common" if is_common else "normal"
        
        # ç®€åŒ–çš„Jobé…ç½®
        job_config = {
            'stage': 'build',
            'tags': ['build'],
            'variables': {
                'PROJECT_PATH': project_path,
                'BOARD': board or "",  # ä¿ç•™åŸå§‹boardåç§°ç”¨äºç›®å½•å
                'SCONS_BOARD': scons_board or "",  # sconså‘½ä»¤ä½¿ç”¨çš„boardåç§°
                'BUILD_ARGS': build_args,
                'LOG_NAME': log_name,
                'PROJECT_TYPE': project_type
            },
            'script': [
                'source ./export.sh',
                'bash tools/ci/build_job.sh'
            ],
            'artifacts': {
                'paths': [f'ci_build_logs/{log_name}.log', f'artifacts/{log_name}/'],
                'expire_in': '1 day',
                'when': 'always'
            },
            'rules': [
                {'if': '$CI_PIPELINE_SOURCE == "parent_pipeline"', 'when': 'always'},
                {'when': 'manual', 'allow_failure': True}
            ]
        }
        return job_name, job_config
    
    def _create_collect_job(self, build_job_names):
        """åˆ›å»ºæ”¶é›†artifactsçš„Job - ä½¿ç”¨åˆ†å±‚ç­–ç•¥é¿å…needsé™åˆ¶"""
        MAX_NEEDS_PER_JOB = 50  # GitLab CIçš„needsé™åˆ¶
        
        collect_jobs = {}
        
        if len(build_job_names) <= MAX_NEEDS_PER_JOB:
            # å¦‚æœjobæ•°é‡ä¸è¶…è¿‡é™åˆ¶ï¼Œç›´æ¥åˆ›å»ºå•ä¸ªæ”¶é›†job
            collect_jobs['collect_all_artifacts'] = {
                'stage': 'collect',
                'tags': ['build'],
                'script': [
                    'echo "ğŸ” å¼€å§‹æ”¶é›†æ‰€æœ‰æ„å»ºartifacts..."',
                    'python3 tools/ci/collect_artifacts_simple.py',
                ],
                'artifacts': {
                    'paths': ['merged_artifacts/'],
                    'expire_in': '1 week',
                    'when': 'always'
                },
                'needs': [{'job': job_name, 'artifacts': True} for job_name in build_job_names],
                'rules': [
                    {'if': '$CI_PIPELINE_SOURCE == "parent_pipeline"', 'when': 'always'}
                ]
            }
        else:
            # å¦‚æœjobæ•°é‡è¶…è¿‡é™åˆ¶ï¼Œä½¿ç”¨åˆ†å±‚æ”¶é›†ç­–ç•¥
            print(f"âš ï¸  æ„å»ºjobæ•°é‡({len(build_job_names)})è¶…è¿‡GitLab CI needsé™åˆ¶({MAX_NEEDS_PER_JOB})ï¼Œä½¿ç”¨åˆ†å±‚æ”¶é›†ç­–ç•¥")
            
            # å°†æ„å»ºjobåˆ†ç»„
            job_groups = []
            for i in range(0, len(build_job_names), MAX_NEEDS_PER_JOB):
                group = build_job_names[i:i + MAX_NEEDS_PER_JOB]
                job_groups.append(group)
            
            intermediate_jobs = []
            
            # ä¸ºæ¯ç»„åˆ›å»ºä¸­é—´æ”¶é›†job
            for group_idx, job_group in enumerate(job_groups):
                intermediate_job_name = f'collect_group_{group_idx + 1}'
                intermediate_jobs.append(intermediate_job_name)
                
                collect_jobs[intermediate_job_name] = {
                    'stage': 'collect',
                    'tags': ['build'],
                    'script': [
                        f'echo "ğŸ” æ”¶é›†ç¬¬{group_idx + 1}ç»„artifacts (å…±{len(job_groups)}ç»„)..."',
                        f'mkdir -p group_{group_idx + 1}_artifacts',
                        'python3 tools/ci/collect_artifacts_simple.py',
                        f'mv merged_artifacts group_{group_idx + 1}_artifacts/',
                        f'echo "âœ… ç¬¬{group_idx + 1}ç»„æ”¶é›†å®Œæˆ"'
                    ],
                    'artifacts': {
                        'paths': [f'group_{group_idx + 1}_artifacts/'],
                        'expire_in': '1 day',
                        'when': 'always'
                    },
                    'needs': [{'job': job_name, 'artifacts': True} for job_name in job_group],
                    'rules': [
                        {'if': '$CI_PIPELINE_SOURCE == "parent_pipeline"', 'when': 'always'}
                    ]
                }
            
            # åˆ›å»ºæœ€ç»ˆåˆå¹¶job
            collect_jobs['merge_all_artifacts'] = {
                'stage': 'collect',
                'tags': ['build'],
                'script': [
                    'echo "ğŸ”„ åˆå¹¶æ‰€æœ‰ç»„çš„artifacts..."',
                    'mkdir -p final_merged_artifacts',
                    'python3 tools/ci/merge_group_artifacts.py',
                ],
                'artifacts': {
                    'paths': [
                        'final_merged_artifacts/'
                    ],
                    'expire_in': '1 week',
                    'when': 'always'
                },
                'needs': [{'job': job_name, 'artifacts': True} for job_name in intermediate_jobs],
                'rules': [
                    {'if': '$CI_PIPELINE_SOURCE == "parent_pipeline"', 'when': 'always'}
                ]
            }
        
        return collect_jobs
    
    def _generate_job_name(self, project_path, board=None):
        """ç”ŸæˆJobåç§°"""
        name = project_path.replace('/', '_').replace('-', '_').replace('.', '_')
        if board:
            name += f"_{board.replace('-', '_')}"
        return f"build_{name}"
    
    def generate_child_pipeline(self):
        """ç”Ÿæˆå­Pipelineé…ç½®"""
        print("ğŸš€ åŠ¨æ€ç”Ÿæˆæ„å»ºPipeline...")
        
        jobs = {}
        job_count = 0
        
        # å¤„ç†æ™®é€šé¡¹ç›®
        if 'projects' in self.config:
            for project_group in self.config['projects']:
                for project_path in project_group['path']:
                    job_name, job_config = self._create_build_job(project_path, is_common=False)
                    jobs[job_name] = job_config
                    job_count += 1
        
        # å¤„ç†é€šç”¨é¡¹ç›®
        if 'common_projects' in self.config:
            for common_project in self.config['common_projects']:
                project_path = common_project['path']
                for board in common_project['boards']:
                    job_name, job_config = self._create_build_job(project_path, board, is_common=True)
                    jobs[job_name] = job_config
                    job_count += 1
        
        # åˆå¹¶é…ç½®
        self.pipeline_config.update(jobs)
        
        # æ·»åŠ æ”¶é›†artifactsçš„job
        if job_count > 0:
            collect_jobs = self._create_collect_job(list(jobs.keys()))
            self.pipeline_config.update(collect_jobs)
            collect_job_count = len(collect_jobs)
        else:
            collect_job_count = 0
        
        print(f"ğŸ“Š åŠ¨æ€ç”Ÿæˆäº† {job_count} ä¸ªæ„å»ºJob + {collect_job_count}ä¸ªæ”¶é›†Job")
        return self.pipeline_config
    
    def save_child_pipeline(self, output_file='child-pipeline.yml'):
        """ä¿å­˜å­Pipelineé…ç½®"""
        pipeline = self.generate_child_pipeline()
        
        with open(output_file, 'w', encoding='utf-8') as f:
            yaml.dump(pipeline, f, default_flow_style=False, allow_unicode=True, indent=2)
        
        print(f"âœ… å­Pipelineé…ç½®å·²ä¿å­˜: {output_file}")
        return output_file
    
    def trigger_child_pipeline(self):
        """è§¦å‘å­Pipeline"""
        # ç”Ÿæˆä¸´æ—¶é…ç½®æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.yml', delete=False) as f:
            pipeline = self.generate_child_pipeline()
            yaml.dump(pipeline, f, default_flow_style=False, allow_unicode=True, indent=2)
            temp_file = f.name
        
        print(f"ğŸ“„ ä¸´æ—¶Pipelineé…ç½®: {temp_file}")
        
        # ä½¿ç”¨GitLab CI APIè§¦å‘å­Pipeline
        if 'CI_PROJECT_ID' in os.environ and 'CI_PIPELINE_ID' in os.environ:
            self._trigger_via_api(temp_file)
        else:
            print("ğŸ’¡ åœ¨æœ¬åœ°ç¯å¢ƒä¸­ï¼Œå°†é…ç½®æ–‡ä»¶å¤åˆ¶åˆ° child-pipeline.yml")
            import shutil
            shutil.copy2(temp_file, 'child-pipeline.yml')
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(temp_file)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='åŠ¨æ€Pipelineç”Ÿæˆå™¨')
    parser.add_argument('-c', '--config', default='build_config_gcc.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', default='child-pipeline.yml', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--trigger', action='store_true', help='ç›´æ¥è§¦å‘å­Pipeline')
    parser.add_argument('--dry-run', action='store_true', help='ä»…æ˜¾ç¤ºç”Ÿæˆçš„Jobæ•°é‡')
    
    args = parser.parse_args()
    
    try:
        generator = DynamicPipelineGenerator(args.config)
        
        if args.dry_run:
            pipeline = generator.generate_child_pipeline()
            job_count = len([k for k in pipeline.keys() if k.startswith('build_')])
            print(f"ğŸ“Š å°†ç”Ÿæˆ {job_count} ä¸ªæ„å»ºJob")
            
        elif args.trigger:
            generator.trigger_child_pipeline()
            
        else:
            generator.save_child_pipeline(args.output)
            
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()
