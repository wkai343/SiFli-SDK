#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import shutil
import time
from pathlib import Path
import glob


class SimpleArtifactsCollector:
    def __init__(self):
        self.merged_dir = Path('merged_artifacts')
        
        # ç¡®ä¿æ ¹ç›®å½•å­˜åœ¨
        self.merged_dir.mkdir(parents=True, exist_ok=True)
    
    def collect_local_artifacts(self):
        collected_jobs = {}
        
        # æ”¶é›†æ—¥å¿—æ–‡ä»¶å¹¶æ˜ å°„åˆ°å¯¹åº”çš„job
        log_pattern = "ci_build_logs/*.log"
        log_files = glob.glob(log_pattern, recursive=True)
        
        print(f"ğŸ“„ å‘ç° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")
        for log_file in log_files:
            log_path = Path(log_file)
            job_name = log_path.stem  # å»æ‰.logåç¼€
            
            # ä¸ºæ¯ä¸ªjobåˆ›å»ºç›®å½•
            job_dir = self.merged_dir / job_name
            job_dir.mkdir(exist_ok=True)
            
            # åˆ›å»ºci_build_logså­ç›®å½•
            logs_subdir = job_dir / 'ci_build_logs'
            logs_subdir.mkdir(exist_ok=True)
            
            # å¤åˆ¶æ—¥å¿—æ–‡ä»¶
            dest_file = logs_subdir / log_path.name
            shutil.copy2(log_file, dest_file)
            print(f"  ğŸ“„ å¤åˆ¶: {job_name}/ci_build_logs/{log_path.name}")
            
            if job_name not in collected_jobs:
                collected_jobs[job_name] = {'logs': 0, 'artifacts': 0}
            collected_jobs[job_name]['logs'] += 1
        
        # æ”¶é›†artifactsç›®å½•ä¸­çš„æ–‡ä»¶
        artifacts_base = Path('artifacts')
        if artifacts_base.exists():
            for job_artifacts_dir in artifacts_base.iterdir():
                if job_artifacts_dir.is_dir():
                    job_name = job_artifacts_dir.name
                    
                    # ä¸ºæ¯ä¸ªjobåˆ›å»ºç›®å½•
                    job_dir = self.merged_dir / job_name
                    job_dir.mkdir(exist_ok=True)
                    
                    # åˆ›å»ºartifactså­ç›®å½•
                    artifacts_subdir = job_dir / 'artifacts'
                    artifacts_subdir.mkdir(exist_ok=True)
                    
                    # å¤åˆ¶æ•´ä¸ªartifactså­ç›®å½•å†…å®¹
                    dest_job_artifacts = artifacts_subdir / job_name
                    if dest_job_artifacts.exists():
                        shutil.rmtree(dest_job_artifacts)
                    shutil.copytree(job_artifacts_dir, dest_job_artifacts)
                    
                    # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
                    file_count = len(list(dest_job_artifacts.rglob('*')))
                    print(f"  ğŸ“¦ å¤åˆ¶: {job_name}/artifacts/{job_name}/ ({file_count} ä¸ªæ–‡ä»¶)")
                    
                    if job_name not in collected_jobs:
                        collected_jobs[job_name] = {'logs': 0, 'artifacts': 0}
                    collected_jobs[job_name]['artifacts'] = file_count
        
        return collected_jobs
    
    def create_summary_report(self, collected_jobs):
        report_path = self.merged_dir / 'build_summary.txt'
        
        total_logs = sum(job['logs'] for job in collected_jobs.values())
        total_artifacts = sum(job['artifacts'] for job in collected_jobs.values())
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("SDKæ„å»ºæ±‡æ€»æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n")
            f.write(f"Pipeline ID: {os.getenv('CI_PIPELINE_ID', 'N/A')}\n")
            f.write(f"é¡¹ç›®ID: {os.getenv('CI_PROJECT_ID', 'N/A')}\n")
            f.write(f"æ„å»ºæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write(f"æ±‡æ€»ç»Ÿè®¡:\n")
            f.write(f"  - æ„å»ºä»»åŠ¡æ•°: {len(collected_jobs)} ä¸ª\n")
            f.write(f"  - æ„å»ºæ—¥å¿—: {total_logs} ä¸ª\n")
            f.write(f"  - æ„å»ºäº§ç‰©: {total_artifacts} ä¸ª\n\n")
            
            f.write("å„Jobè¯¦ç»†ç»Ÿè®¡:\n")
            f.write("-" * 50 + "\n")
            for job_name, stats in sorted(collected_jobs.items()):
                f.write(f"{job_name}:\n")
                f.write(f"  - æ—¥å¿—æ–‡ä»¶: {stats['logs']} ä¸ª\n")
                f.write(f"  - æ„å»ºäº§ç‰©: {stats['artifacts']} ä¸ª\n")
                
                # åˆ—å‡ºå…·ä½“æ–‡ä»¶
                job_dir = self.merged_dir / job_name
                if job_dir.exists():
                    log_files = list((job_dir / 'ci_build_logs').glob('*.log')) if (job_dir / 'ci_build_logs').exists() else []
                    if log_files:
                        f.write(f"  æ—¥å¿—æ–‡ä»¶åˆ—è¡¨:\n")
                        for log_file in sorted(log_files):
                            size = log_file.stat().st_size
                            f.write(f"    - {log_file.name} ({size:,} bytes)\n")
                    
                    artifacts_dir = job_dir / 'artifacts' / job_name
                    if artifacts_dir.exists():
                        artifact_files = list(artifacts_dir.rglob('*'))
                        artifact_files = [f for f in artifact_files if f.is_file()]
                        if artifact_files:
                            f.write(f"  æ„å»ºäº§ç‰©åˆ—è¡¨:\n")
                            for artifact_file in sorted(artifact_files):
                                size = artifact_file.stat().st_size
                                rel_path = artifact_file.relative_to(artifacts_dir)
                                f.write(f"    - {rel_path} ({size:,} bytes)\n")
                f.write("\n")
            
            # è®¡ç®—æ€»å¤§å°
            total_size = sum(f.stat().st_size for f in self.merged_dir.rglob('*') if f.is_file())
            f.write(f"æ€»å¤§å°: {total_size:,} bytes ({total_size / (1024*1024):.2f} MB)\n")
        
        print(f"ğŸ“„ æ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    def create_file_index(self, collected_jobs):
        index_path = self.merged_dir / 'file_index.json'
        
        import json
        
        index_data = {
            "generated_at": time.strftime('%Y-%m-%d %H:%M:%S'),
            "pipeline_id": os.getenv('CI_PIPELINE_ID', 'N/A'),
            "project_id": os.getenv('CI_PROJECT_ID', 'N/A'),
            "jobs": {}
        }
        
        # ä¸ºæ¯ä¸ªjobåˆ›å»ºç´¢å¼•
        for job_name in collected_jobs.keys():
            job_dir = self.merged_dir / job_name
            if not job_dir.exists():
                continue
                
            job_index = {
                "logs": [],
                "artifacts": []
            }
            
            # ç´¢å¼•æ—¥å¿—æ–‡ä»¶
            logs_dir = job_dir / 'ci_build_logs'
            if logs_dir.exists():
                for log_file in sorted(logs_dir.glob('*.log')):
                    job_index["logs"].append({
                        "name": log_file.name,
                        "size": log_file.stat().st_size,
                        "path": f"{job_name}/ci_build_logs/{log_file.name}"
                    })
            
            # ç´¢å¼•æ„å»ºäº§ç‰©
            artifacts_dir = job_dir / 'artifacts' / job_name
            if artifacts_dir.exists():
                for artifact_file in sorted(artifacts_dir.rglob('*')):
                    if artifact_file.is_file():
                        rel_path = artifact_file.relative_to(artifacts_dir)
                        job_index["artifacts"].append({
                            "name": artifact_file.name,
                            "size": artifact_file.stat().st_size,
                            "path": f"{job_name}/artifacts/{job_name}/{rel_path}",
                            "type": artifact_file.suffix.lstrip('.')
                        })
            
            index_data["jobs"][job_name] = job_index
        
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ æ–‡ä»¶ç´¢å¼•å·²ç”Ÿæˆ: {index_path}")
    
    def collect_artifacts(self):
        # æ˜¾ç¤ºå½“å‰ç›®å½•ç»“æ„ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        print("ğŸ“ å½“å‰ç›®å½•ç»“æ„:")
        for item in sorted(Path('.').rglob('*')):
            if item.is_file() and any(str(item).startswith(prefix) for prefix in ['ci_build_logs', 'artifacts']):
                print(f"  {item}")
        
        # æ”¶é›†æœ¬åœ°artifactså¹¶ä¿æŒåŸå§‹ç›®å½•ç»“æ„
        collected_jobs = self.collect_local_artifacts()
        
        # åˆ›å»ºæŠ¥å‘Šå’Œç´¢å¼•
        self.create_summary_report(collected_jobs)
        self.create_file_index(collected_jobs)
        
        total_jobs = len(collected_jobs)
        total_logs = sum(job['logs'] for job in collected_jobs.values())
        total_artifacts = sum(job['artifacts'] for job in collected_jobs.values())
        
        print(f"\nâœ… artifactsæ”¶é›†å®Œæˆ!")
        print(f"ğŸ“Š ç»Ÿè®¡: {total_jobs} ä¸ªjob, {total_logs} ä¸ªæ—¥å¿—æ–‡ä»¶, {total_artifacts} ä¸ªæ„å»ºäº§ç‰©")
        
        # æ˜¾ç¤ºåˆå¹¶åçš„ç›®å½•ç»“æ„
        print(f"\nğŸ“ åˆå¹¶åçš„artifactsç»“æ„:")
        for job_dir in sorted(self.merged_dir.iterdir()):
            if job_dir.is_dir() and job_dir.name not in ['build_summary.txt', 'file_index.json']:
                print(f"  ğŸ“‚ {job_dir.name}/")
                
                # æ˜¾ç¤ºci_build_logsç›®å½•
                logs_dir = job_dir / 'ci_build_logs'
                if logs_dir.exists():
                    log_files = list(logs_dir.glob('*.log'))
                    if log_files:
                        print(f"    ğŸ“‚ ci_build_logs/ ({len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶)")
                
                # æ˜¾ç¤ºartifactsç›®å½•
                artifacts_dir = job_dir / 'artifacts'
                if artifacts_dir.exists():
                    artifact_files = list(artifacts_dir.rglob('*'))
                    artifact_files = [f for f in artifact_files if f.is_file()]
                    if artifact_files:
                        print(f"    ğŸ“‚ artifacts/ ({len(artifact_files)} ä¸ªæ„å»ºäº§ç‰©)")
        
        # æ˜¾ç¤ºæ±‡æ€»æ–‡ä»¶
        for summary_file in ['build_summary.txt', 'file_index.json']:
            summary_path = self.merged_dir / summary_file
            if summary_path.exists():
                size = summary_path.stat().st_size
                print(f"  ğŸ“„ {summary_file} ({size:,} bytes)")


def main():
    try:
        collector = SimpleArtifactsCollector()
        collector.collect_artifacts()
        print("\nğŸ‰ Artifactsæ”¶é›†æˆåŠŸå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ æ”¶é›†artifactså¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
