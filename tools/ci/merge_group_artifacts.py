#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆå¹¶åˆ†ç»„artifactsçš„è„šæœ¬
ç”¨äºå°†å¤šä¸ªä¸­é—´æ”¶é›†jobçš„artifactsåˆå¹¶ä¸ºæœ€ç»ˆç»“æœ
"""

import os
import sys
import shutil
import time
import json
from pathlib import Path
import glob


class GroupArtifactsMerger:
    def __init__(self):
        self.final_dir = Path('final_merged_artifacts')
        self.final_dir.mkdir(exist_ok=True)
    
    def merge_group_artifacts(self):
        """åˆå¹¶æ‰€æœ‰ç»„çš„artifacts"""
        print("ğŸ”„ å¼€å§‹åˆå¹¶åˆ†ç»„artifacts...")
        
        # æŸ¥æ‰¾æ‰€æœ‰ç»„artifactsç›®å½•
        group_dirs = glob.glob('group_*_artifacts')
        group_dirs.sort()
        
        if not group_dirs:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»„artifactsç›®å½•")
            return False
        
        print(f"ğŸ“ å‘ç° {len(group_dirs)} ä¸ªç»„artifactsç›®å½•")
        
        merged_jobs = {}
        total_jobs = 0
        total_logs = 0
        total_artifacts = 0
        
        # åˆå¹¶æ¯ä¸ªç»„çš„å†…å®¹
        for group_dir in group_dirs:
            group_path = Path(group_dir)
            merged_artifacts_path = group_path / 'merged_artifacts'
            
            if not merged_artifacts_path.exists():
                print(f"âš ï¸  è·³è¿‡ {group_dir}ï¼šæ²¡æœ‰merged_artifactsç›®å½•")
                continue
            
            print(f"ğŸ“¦ å¤„ç† {group_dir}...")
            
            # å¤åˆ¶æ‰€æœ‰jobç›®å½•åˆ°æœ€ç»ˆç›®å½•
            for job_dir in merged_artifacts_path.iterdir():
                if job_dir.is_dir() and not job_dir.name.endswith('.txt') and not job_dir.name.endswith('.json'):
                    dest_job_dir = self.final_dir / job_dir.name
                    
                    if dest_job_dir.exists():
                        print(f"âš ï¸  ç›®å½• {job_dir.name} å·²å­˜åœ¨ï¼Œè·³è¿‡")
                        continue
                    
                    shutil.copytree(job_dir, dest_job_dir)
                    print(f"  ğŸ“‚ å¤åˆ¶: {job_dir.name}")
                    
                    # ç»Ÿè®¡è¿™ä¸ªjobçš„æ–‡ä»¶
                    job_stats = self._count_job_files(dest_job_dir)
                    merged_jobs[job_dir.name] = job_stats
                    total_jobs += 1
                    total_logs += job_stats['logs']
                    total_artifacts += job_stats['artifacts']
        
        # åˆ›å»ºæœ€ç»ˆçš„æ±‡æ€»æŠ¥å‘Š
        self._create_final_summary(merged_jobs, total_jobs, total_logs, total_artifacts)
        
        # åˆ›å»ºæœ€ç»ˆçš„æ–‡ä»¶ç´¢å¼•
        self._create_final_index(merged_jobs)
        
        print(f"âœ… åˆå¹¶å®Œæˆ: {total_jobs} ä¸ªjob, {total_logs} ä¸ªæ—¥å¿—, {total_artifacts} ä¸ªæ„å»ºäº§ç‰©")
        return True
    
    def _count_job_files(self, job_dir):
        """ç»Ÿè®¡å•ä¸ªjobç›®å½•ä¸­çš„æ–‡ä»¶æ•°é‡"""
        logs_count = 0
        artifacts_count = 0
        
        # ç»Ÿè®¡æ—¥å¿—æ–‡ä»¶
        logs_dir = job_dir / 'ci_build_logs'
        if logs_dir.exists():
            logs_count = len(list(logs_dir.glob('*.log')))
        
        # ç»Ÿè®¡æ„å»ºäº§ç‰©
        artifacts_dir = job_dir / 'artifacts'
        if artifacts_dir.exists():
            artifact_files = [f for f in artifacts_dir.rglob('*') if f.is_file()]
            artifacts_count = len(artifact_files)
        
        return {'logs': logs_count, 'artifacts': artifacts_count}
    
    def _create_final_summary(self, merged_jobs, total_jobs, total_logs, total_artifacts):
        """åˆ›å»ºæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š"""
        print("ğŸ“Š åˆ›å»ºæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š...")
        
        report_path = self.final_dir / 'build_summary.txt'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("SDKæ„å»ºæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n")
            f.write(f"Pipeline ID: {os.getenv('CI_PIPELINE_ID', 'N/A')}\n")
            f.write(f"é¡¹ç›®ID: {os.getenv('CI_PROJECT_ID', 'N/A')}\n")
            f.write(f"åˆå¹¶æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write(f"æ€»ä½“ç»Ÿè®¡:\n")
            f.write(f"  - æ„å»ºä»»åŠ¡æ•°: {total_jobs} ä¸ª\n")
            f.write(f"  - æ„å»ºæ—¥å¿—: {total_logs} ä¸ª\n")
            f.write(f"  - æ„å»ºäº§ç‰©: {total_artifacts} ä¸ª\n\n")
            
            f.write("å„Jobè¯¦ç»†ç»Ÿè®¡:\n")
            f.write("-" * 50 + "\n")
            
            for job_name, stats in sorted(merged_jobs.items()):
                f.write(f"{job_name}:\n")
                f.write(f"  - æ—¥å¿—æ–‡ä»¶: {stats['logs']} ä¸ª\n")
                f.write(f"  - æ„å»ºäº§ç‰©: {stats['artifacts']} ä¸ª\n")
                
                # åˆ—å‡ºå…·ä½“æ–‡ä»¶
                job_dir = self.final_dir / job_name
                if job_dir.exists():
                    logs_dir = job_dir / 'ci_build_logs'
                    if logs_dir.exists():
                        log_files = list(logs_dir.glob('*.log'))
                        if log_files:
                            f.write(f"  æ—¥å¿—æ–‡ä»¶åˆ—è¡¨:\n")
                            for log_file in sorted(log_files):
                                size = log_file.stat().st_size
                                f.write(f"    - {log_file.name} ({size:,} bytes)\n")
                    
                    artifacts_dir = job_dir / 'artifacts'
                    if artifacts_dir.exists():
                        artifact_files = [f for f in artifacts_dir.rglob('*') if f.is_file()]
                        if artifact_files:
                            f.write(f"  æ„å»ºäº§ç‰©åˆ—è¡¨:\n")
                            for artifact_file in sorted(artifact_files):
                                size = artifact_file.stat().st_size
                                rel_path = artifact_file.relative_to(artifacts_dir)
                                f.write(f"    - {rel_path} ({size:,} bytes)\n")
                f.write("\n")
            
            # è®¡ç®—æ€»å¤§å°
            total_size = sum(f.stat().st_size for f in self.final_dir.rglob('*') if f.is_file())
            f.write(f"æ€»å¤§å°: {total_size:,} bytes ({total_size / (1024*1024):.2f} MB)\n")
        
        print(f"ğŸ“„ æœ€ç»ˆæ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    def _create_final_index(self, merged_jobs):
        """åˆ›å»ºæœ€ç»ˆæ–‡ä»¶ç´¢å¼•"""
        index_path = self.final_dir / 'file_index.json'
        
        index_data = {
            "generated_at": time.strftime('%Y-%m-%d %H:%M:%S'),
            "pipeline_id": os.getenv('CI_PIPELINE_ID', 'N/A'),
            "project_id": os.getenv('CI_PROJECT_ID', 'N/A'),
            "merge_strategy": "group_based",
            "total_jobs": len(merged_jobs),
            "jobs": {}
        }
        
        # ä¸ºæ¯ä¸ªjobåˆ›å»ºç´¢å¼•
        for job_name in merged_jobs.keys():
            job_dir = self.final_dir / job_name
            if not job_dir.exists():
                continue
                
            job_index = {
                "logs": [],
                "artifacts": []
            }
            
            # ç´¢å¼•æ—¥å¿—æ–‡ä»¶
            logs_dir = job_dir / 'ci_build_logs'
            if logs_dir.exists():
                for log_file in sorted(logs_dir.glob('*.log')):
                    job_index["logs"].append({
                        "name": log_file.name,
                        "size": log_file.stat().st_size,
                        "path": f"{job_name}/ci_build_logs/{log_file.name}"
                    })
            
            # ç´¢å¼•æ„å»ºäº§ç‰©
            artifacts_dir = job_dir / 'artifacts'
            if artifacts_dir.exists():
                for artifact_file in sorted(artifacts_dir.rglob('*')):
                    if artifact_file.is_file():
                        rel_path = artifact_file.relative_to(artifacts_dir)
                        job_index["artifacts"].append({
                            "name": artifact_file.name,
                            "size": artifact_file.stat().st_size,
                            "path": f"{job_name}/artifacts/{rel_path}",
                            "type": artifact_file.suffix.lstrip('.')
                        })
            
            index_data["jobs"][job_name] = job_index
        
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ æœ€ç»ˆæ–‡ä»¶ç´¢å¼•å·²ç”Ÿæˆ: {index_path}")


def main():
    """ä¸»å‡½æ•°"""
    try:
        merger = GroupArtifactsMerger()
        success = merger.merge_group_artifacts()
        
        if success:
            print("\nğŸ‰ åˆ†ç»„artifactsåˆå¹¶æˆåŠŸå®Œæˆï¼")
        else:
            print("\nâŒ åˆ†ç»„artifactsåˆå¹¶å¤±è´¥")
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ åˆå¹¶åˆ†ç»„artifactså¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
